{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ***SETUP***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import faiss\n",
        "import numpy as np\n",
        "import gc\n",
        "import time\n",
        "from fastapi import FastAPI\n",
        "from fastapi.responses import HTMLResponse, JSONResponse\n",
        "from pathlib import Path\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "# Load environment variables from .env\n",
        "load_dotenv()\n",
        "gemini_flash_api_key = os.getenv(\"FlashAPI\")\n",
        "mongo_uri = os.getenv(\"MONGO_URI\")\n",
        "index_uri = os.getenv(\"INDEX_URI\")\n",
        "if not gemini_flash_api_key:\n",
        "    raise ValueError(\"❌ Gemini Flash API key (FlashAPI) is missing!\")\n",
        "if not mongo_uri:\n",
        "    raise ValueError(\"❌ MongoDB URI (MongoURI) is missing!\")\n",
        "if not index_uri:\n",
        "    raise ValueError(\"❌ INDEX_URI for FAISS index cluster is missing!\")\n",
        "\n",
        "# --- Environment variables to mitigate segmentation faults ---\n",
        "os.environ[\"OMP_NUM_THREADS\"] = \"1\"\n",
        "os.environ[\"MKL_NUM_THREADS\"] = \"1\"\n",
        "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
        "\n",
        "# --- Setup local project directory (for model cache) ---\n",
        "project_dir = \"./AutoGenRAGMedicalChatbot\"\n",
        "os.makedirs(project_dir, exist_ok=True)\n",
        "huggingface_cache_dir = os.path.join(project_dir, \"huggingface_models\")\n",
        "os.environ[\"HF_HOME\"] = huggingface_cache_dir  # Use this folder for HF cache\n",
        "\n",
        "# --- Download (or load from cache) the SentenceTransformer model ---\n",
        "from huggingface_hub import snapshot_download\n",
        "print(\"Checking or downloading the all-MiniLM-L6-v2 model from huggingface_hub...\")\n",
        "model_loc = snapshot_download(\n",
        "    repo_id=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
        "    cache_dir=os.environ[\"HF_HOME\"],\n",
        "    local_files_only=False\n",
        ")\n",
        "print(f\"Model directory: {model_loc}\")\n",
        "\n",
        "from sentence_transformers import SentenceTransformer\n",
        "embedding_model = SentenceTransformer(model_loc, device=\"cpu\")\n",
        "\n",
        "# --- MongoDB Setup ---\n",
        "from pymongo import MongoClient\n",
        "# QA client\n",
        "client = MongoClient(mongo_uri)\n",
        "db = client[\"MedicalChatbotDB\"]  # Use your chosen database name\n",
        "qa_collection = db[\"qa_data\"]\n",
        "\n",
        "# FAISS index client\n",
        "iclient = MongoClient(index_uri)\n",
        "idb = iclient[\"FAISSIndexCluster\"]  # Use your chosen database name\n",
        "index_collection = idb[\"faiss_index\"]"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# QA Embedding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- Load or Build QA Data in MongoDB ---\n",
        "print(\"✅ Checking MongoDB for existing QA data...\")\n",
        "if qa_collection.count_documents({}) == 0:\n",
        "    print(\"⚠️ QA data not found in MongoDB. Loading dataset from Hugging Face...\")\n",
        "    from datasets import load_dataset\n",
        "    dataset = load_dataset(\"ruslanmv/ai-medical-chatbot\", cache_dir=huggingface_cache_dir)\n",
        "    df = dataset[\"train\"].to_pandas()[[\"Patient\", \"Doctor\"]]\n",
        "    # Add an index column \"i\" to preserve order.\n",
        "    df[\"i\"] = range(len(df))\n",
        "    qa_data = df.to_dict(\"records\")\n",
        "    # Insert in batches (e.g., batches of 1000) to avoid document size limits.\n",
        "    batch_size = 1000\n",
        "    for i in range(0, len(qa_data), batch_size):\n",
        "        qa_collection.insert_many(qa_data[i:i+batch_size])\n",
        "    print(f\"✅ QA data stored in MongoDB. Total entries: {len(qa_data)}\")\n",
        "else:\n",
        "    print(\"✅ Loaded existing QA data from MongoDB.\")\n",
        "    # Use an aggregation pipeline with allowDiskUse to sort by \"i\" without creating an index.\n",
        "    qa_docs = list(qa_collection.aggregate([\n",
        "        {\"$sort\": {\"i\": 1}},\n",
        "        {\"$project\": {\"_id\": 0}}\n",
        "    ], allowDiskUse=True))\n",
        "    qa_data = qa_docs\n",
        "    print(\"Total QA entries loaded:\", len(qa_data))\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# FAISS Index Embedding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- Build FAISS Index ---\n",
        "print(\"Building a compressed FAISS index (using IVFPQ) from QA data...\")\n",
        "# Compute embeddings for each QA pair by concatenating \"Patient\" and \"Doctor\" fields.\n",
        "texts = [doc.get(\"Patient\", \"\") + \" \" + doc.get(\"Doctor\", \"\") for doc in qa_data]\n",
        "print(\"Total texts to embed:\", len(texts))\n",
        "\n",
        "batch_size = 512\n",
        "embeddings_list = []\n",
        "for i in range(0, len(texts), batch_size):\n",
        "    batch = texts[i: i + batch_size]\n",
        "    batch_embeddings = embedding_model.encode(batch, convert_to_numpy=True).astype(np.float32)\n",
        "    embeddings_list.append(batch_embeddings)\n",
        "    print(f\"Encoded batch {i} to {i + len(batch)}\")\n",
        "\n",
        "embeddings = np.vstack(embeddings_list)\n",
        "dim = embeddings.shape[1]\n",
        "print(\"Embeddings shape:\", embeddings.shape)\n",
        "\n",
        "# --- Build Compressed FAISS Index using IVFPQ (to reduce storage size) ---\n",
        "nlist = 100   # number of clusters\n",
        "m = 8         # number of subquantizers\n",
        "nbits = 8     # bits per subvector\n",
        "\n",
        "quantizer = faiss.IndexFlatL2(dim)\n",
        "index = faiss.IndexIVFPQ(quantizer, dim, nlist, m, nbits)\n",
        "print(\"Training the IVFPQ index on embeddings...\")\n",
        "index.train(embeddings)\n",
        "index.add(embeddings)\n",
        "print(\"Compressed FAISS index built. Total vectors:\", index.ntotal)\n",
        "\n",
        "# --- Serialize and Store FAISS Index in GridFS (on separate cluster) ---\n",
        "print(\"Serializing FAISS index...\")\n",
        "index_bytes = faiss.serialize_index(index)\n",
        "# Convert the raw bytes to a native bytes object\n",
        "index_data = np.frombuffer(index_bytes, dtype='uint8').tobytes()\n",
        "\n",
        "# Delete any existing FAISS index file in GridFS.\n",
        "existing_file = fs.find_one({\"filename\": \"faiss_index.bin\"})\n",
        "if existing_file:\n",
        "    fs.delete(existing_file._id)\n",
        "\n",
        "file_id = fs.put(index_data, filename=\"faiss_index.bin\")\n",
        "print(\"✅ FAISS index stored in GridFS with file_id:\", file_id)\n",
        "\n",
        "del embeddings\n",
        "gc.collect()\n",
        "print(\"✅ Compressed FAISS index stored in MongoDB (separate cluster) successfully!\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "HpO1du0qRRFi",
        "QrueOe0bRrUu",
        "ECmy5I5irsiH"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.12.7 (main, Oct 10 2024, 10:50:01) [Clang 14.0.0 (clang-1400.0.29.202)]"
    },
    "vscode": {
      "interpreter": {
        "hash": "f1062708a37074d70712b695aadee582e0b0b9f95f45576b5521424137d05fec"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
